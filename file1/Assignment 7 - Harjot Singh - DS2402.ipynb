{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7470946c",
   "metadata": {},
   "source": [
    "# Assignment 7 == Exception Handling (Web Scraping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3e811bad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import selenium\n",
    "from selenium import webdriver\n",
    "import time\n",
    "import requests\n",
    "import re\n",
    "from selenium.common.exceptions import NoSuchElementException, StaleElementReferenceException\n",
    "from selenium.webdriver.support.ui import WebDriverWait"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "581e584d",
   "metadata": {},
   "source": [
    "1. Write a python program which searches all the product under a particular product from www.amazon.in. The product to be searched will be taken as input from user. For e.g. If user input is ‘guitar’. Then search for guitars."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4c84677",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "def search_amazon_products():\n",
    "    try:\n",
    "        # Get user input for the product to search\n",
    "        product_name = input(\"Enter the product you want to search for: \")\n",
    "\n",
    "        # Construct the search URL\n",
    "        base_url = \"https://www.amazon.in/s\"\n",
    "        params = {\"k\": product_name}\n",
    "        response = requests.get(base_url, params=params)\n",
    "\n",
    "        # Check if the request was successful\n",
    "        if response.status_code == 200:\n",
    "            # Extract relevant information (you can parse the HTML content here)\n",
    "            print(f\"Search results for '{product_name}':\")\n",
    "            print(response.url)  # Print the URL for reference\n",
    "        else:\n",
    "            print(\"Error fetching search results. Please try again later.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {str(e)}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    search_amazon_products()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7eac5f0",
   "metadata": {},
   "source": [
    "2. In the above question, now scrape the following details of each product listed in first 3 pages of your search results and save it in a data frame and csv. In case if any product has less than 3 pages in search results then scrape all the products available under that product name. Details to be scraped are: \"Brand Name\", \"Name of the Product\", \"Price\", \"Return/Exchange\", \"Expected Delivery\", \"Availability\" and “Product URL”. In case, if any of the details are missing for any of the product then replace it by “-“."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0523b7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "def scrape_amazon_products(product_name, num_pages=3):\n",
    "    try:\n",
    "        base_url = \"https://www.amazon.in/s\"\n",
    "        params = {\"k\": product_name}\n",
    "        product_data = []\n",
    "\n",
    "        for page in range(1, num_pages + 1):\n",
    "            params[\"page\"] = page\n",
    "            response = requests.get(base_url, params=params)\n",
    "            soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "            # Extract product details\n",
    "            for item in soup.find_all(\"div\", class_=\"s-result-item\"):\n",
    "                product = {}\n",
    "                product[\"Brand Name\"] = item.find(\"span\", class_=\"a-size-base-plus\").text.strip()\n",
    "                product[\"Name of the Product\"] = item.find(\"span\", class_=\"a-text-normal\").text.strip()\n",
    "                product[\"Price\"] = item.find(\"span\", class_=\"a-price-whole\").text.strip() if item.find(\"span\", class_=\"a-price-whole\") else \"-\"\n",
    "                product[\"Return/Exchange\"] = item.find(\"span\", class_=\"a-declarative\").text.strip() if item.find(\"span\", class_=\"a-declarative\") else \"-\"\n",
    "                product[\"Expected Delivery\"] = item.find(\"span\", class_=\"a-text-bold\").text.strip() if item.find(\"span\", class_=\"a-text-bold\") else \"-\"\n",
    "                product[\"Availability\"] = item.find(\"span\", class_=\"a-size-base\").text.strip() if item.find(\"span\", class_=\"a-size-base\") else \"-\"\n",
    "                product[\"Product URL\"] = item.find(\"a\", class_=\"a-link-normal\")[\"href\"] if item.find(\"a\", class_=\"a-link-normal\") else \"-\"\n",
    "                product_data.append(product)\n",
    "\n",
    "        # Create a DataFrame\n",
    "        df = pd.DataFrame(product_data)\n",
    "\n",
    "        # Save to CSV\n",
    "        df.to_csv(f\"{product_name}_products.csv\", index=False)\n",
    "        print(f\"Data saved to {product_name}_products.csv\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {str(e)}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    search_product = input(\"Enter the product you want to search for: \")\n",
    "    scrape_amazon_products(search_product, num_pages=3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dabef85a",
   "metadata": {},
   "source": [
    "3. Write a python program to access the search bar and search button on images.google.com and scrape 10 images each for keywords ‘fruits’, ‘cars’ and ‘Machine Learning’, ‘Guitar’, ‘Cakes’."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5f48504",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "def scrape_images(keyword, num_images=10):\n",
    "    try:\n",
    "        # Set up Chrome WebDriver\n",
    "        chrome_options = Options()\n",
    "        chrome_options.add_argument(\"--headless\")  # Run in headless mode (no GUI)\n",
    "        chrome_options.add_argument(\"--disable-gpu\")  # Disable GPU acceleration\n",
    "        chrome_options.add_argument(\"--window-size=1920x1080\")  # Set window size\n",
    "        chrome_service = Service(executable_path=\"chromedriver.exe\")  # Path to chromedriver executable\n",
    "        driver = webdriver.Chrome(service=chrome_service, options=chrome_options)\n",
    "\n",
    "        # Open Google Images\n",
    "        driver.get(\"https://images.google.com/\")\n",
    "        time.sleep(2)\n",
    "\n",
    "        # Find the search bar and enter the keyword\n",
    "        search_bar = driver.find_element(By.NAME, \"q\")\n",
    "        search_bar.send_keys(keyword)\n",
    "        search_bar.send_keys(Keys.RETURN)\n",
    "\n",
    "        # Wait for search results to load\n",
    "        WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.CLASS_NAME, \"rg_i\")))\n",
    "\n",
    "        # Scroll down to load more images\n",
    "        for _ in range(3):\n",
    "            driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "            time.sleep(2)\n",
    "\n",
    "        # Get image elements\n",
    "        image_elements = driver.find_elements(By.CLASS_NAME, \"rg_i\")\n",
    "\n",
    "        # Scrape image URLs\n",
    "        image_urls = []\n",
    "        for i, img in enumerate(image_elements[:num_images]):\n",
    "            img_url = img.get_attribute(\"src\")\n",
    "            if img_url:\n",
    "                image_urls.append(img_url)\n",
    "                print(f\"Image {i+1}: {img_url}\")\n",
    "\n",
    "        # Close the browser\n",
    "        driver.quit()\n",
    "\n",
    "        return image_urls\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {str(e)}\")\n",
    "        driver.quit()\n",
    "        return []\n",
    "\n",
    "# Keywords to search for\n",
    "keywords = [\"fruits\", \"cars\", \"Machine Learning\", \"Guitar\", \"Cakes\"]\n",
    "\n",
    "# Scrape images for each keyword\n",
    "for keyword in keywords:\n",
    "    print(f\"Scraping images for '{keyword}':\")\n",
    "    image_urls = scrape_images(keyword, num_images=10)\n",
    "    print(f\"Total images scraped: {len(image_urls)}\\n\")\n",
    "\n",
    "# Note: You need to have the 'chromedriver.exe' file in the same directory as this script.\n",
    "# You can download it from here.\n",
    "# Make sure it matches your Chrome browser version."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13c3a3a3",
   "metadata": {},
   "source": [
    "4. Write a python program to search for a smartphone(e.g.: Oneplus Nord, pixel 4A, etc.) on www.flipkart.com and scrape following details for all the search results displayed on 1st page. Details to be scraped: “Brand Name”, “Smartphone name”, “Colour”, “RAM”, “Storage(ROM)”, “Primary Camera”, “Secondary Camera”, “Display Size”, “Battery Capacity”, “Price”, “Product URL”. Incase if any of the details is missing then replace it by “- “. Save your results in a dataframe and CSV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8643efa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "def scrape_flipkart_smartphones(keyword, num_results=10):\n",
    "    try:\n",
    "        base_url = \"https://www.flipkart.com\"\n",
    "        search_url = f\"{base_url}/search?q={keyword}&otracker=search&otracker1=search&marketplace=FLIPKART&as-show=on&as=off\"\n",
    "        response = requests.get(search_url)\n",
    "        soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "        # Extract product details\n",
    "        product_data = []\n",
    "        for item in soup.find_all(\"div\", class_=\"_1AtVbE\"):\n",
    "            product = {}\n",
    "            product[\"Brand Name\"] = item.find(\"div\", class_=\"_4rR01T\").text.strip()\n",
    "            product[\"Smartphone Name\"] = item.find(\"a\", class_=\"IRpwTa\").text.strip()\n",
    "            product[\"Colour\"] = item.find(\"div\", class_=\"rgWa7D\").text.strip() if item.find(\"div\", class_=\"rgWa7D\") else \"-\"\n",
    "            product[\"RAM\"] = item.find_all(\"li\", class_=\"rgWa7D\")[0].text.strip() if item.find_all(\"li\", class_=\"rgWa7D\") else \"-\"\n",
    "            product[\"Storage (ROM)\"] = item.find_all(\"li\", class_=\"rgWa7D\")[1].text.strip() if item.find_all(\"li\", class_=\"rgWa7D\") else \"-\"\n",
    "            product[\"Primary Camera\"] = item.find_all(\"li\", class_=\"rgWa7D\")[2].text.strip() if item.find_all(\"li\", class_=\"rgWa7D\") else \"-\"\n",
    "            product[\"Secondary Camera\"] = item.find_all(\"li\", class_=\"rgWa7D\")[3].text.strip() if item.find_all(\"li\", class_=\"rgWa7D\") else \"-\"\n",
    "            product[\"Display Size\"] = item.find_all(\"li\", class_=\"rgWa7D\")[4].text.strip() if item.find_all(\"li\", class_=\"rgWa7D\") else \"-\"\n",
    "            product[\"Battery Capacity\"] = item.find_all(\"li\", class_=\"rgWa7D\")[5].text.strip() if item.find_all(\"li\", class_=\"rgWa7D\") else \"-\"\n",
    "            product[\"Price\"] = item.find(\"div\", class_=\"_30jeq3\").text.strip() if item.find(\"div\", class_=\"_30jeq3\") else \"-\"\n",
    "            product[\"Product URL\"] = base_url + item.find(\"a\", class_=\"IRpwTa\")[\"href\"]\n",
    "            product_data.append(product)\n",
    "\n",
    "            if len(product_data) >= num_results:\n",
    "                break\n",
    "\n",
    "        # Create a DataFrame\n",
    "        df = pd.DataFrame(product_data)\n",
    "\n",
    "        # Save to CSV\n",
    "        df.to_csv(f\"{keyword}_smartphones.csv\", index=False)\n",
    "        print(f\"Data saved to {keyword}_smartphones.csv\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {str(e)}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    search_keyword = input(\"Enter the smartphone you want to search for: \")\n",
    "    scrape_flipkart_smartphones(search_keyword, num_results=10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73882ef9",
   "metadata": {},
   "source": [
    "5. Write a program to scrap geospatial coordinates (latitude, longitude) of a city searched on google maps.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db71d942",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "def get_coordinates(city_name):\n",
    "    try:\n",
    "        # Construct the search URL for Google Maps Geocoding API\n",
    "        base_url = \"https://maps.googleapis.com/maps/api/geocode/json\"\n",
    "        params = {\"address\": city_name, \"key\": \"YOUR_API_KEY\"}  # Replace with your API key\n",
    "        response = requests.get(base_url, params=params)\n",
    "        data = response.json()\n",
    "\n",
    "        # Check if the request was successful\n",
    "        if response.status_code == 200 and data.get(\"results\"):\n",
    "            location = data[\"results\"][0][\"geometry\"][\"location\"]\n",
    "            latitude = location[\"lat\"]\n",
    "            longitude = location[\"lng\"]\n",
    "            return latitude, longitude\n",
    "        else:\n",
    "            print(\"Error fetching coordinates. Please check your API key or try again later.\")\n",
    "            return None, None\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {str(e)}\")\n",
    "        return None, None\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    city = input(\"Enter the city name: \")\n",
    "    lat, lng = get_coordinates(city)\n",
    "    if lat and lng:\n",
    "        print(f\"Coordinates for {city}: Latitude {lat}, Longitude {lng}\")\n",
    "    else:\n",
    "        print(\"Coordinates not found.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6ee9ec7",
   "metadata": {},
   "source": [
    "6. Write a program to scrap all the available details of best gaming laptops from digit.in.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdce9b50",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "def scrape_gaming_laptops(url):\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "        # Find all gaming laptop details\n",
    "        laptop_details = []\n",
    "        for item in soup.find_all(\"div\", class_=\"TopNumbeHeading\"):\n",
    "            laptop = {}\n",
    "            laptop[\"Brand Name\"] = item.find(\"h3\").text.strip()\n",
    "            laptop[\"Smartphone Name\"] = item.find(\"h2\").text.strip()\n",
    "            laptop[\"Colour\"] = item.find(\"span\", class_=\"color\").text.strip()\n",
    "            laptop[\"RAM\"] = item.find(\"span\", class_=\"ram\").text.strip()\n",
    "            laptop[\"Storage(ROM)\"] = item.find(\"span\", class_=\"rom\").text.strip()\n",
    "            laptop[\"Primary Camera\"] = item.find(\"span\", class_=\"camera\").text.strip()\n",
    "            laptop[\"Secondary Camera\"] = item.find(\"span\", class_=\"secondary-camera\").text.strip()\n",
    "            laptop[\"Display Size\"] = item.find(\"span\", class_=\"display\").text.strip()\n",
    "            laptop[\"Battery Capacity\"] = item.find(\"span\", class_=\"battery\").text.strip()\n",
    "            laptop[\"Price\"] = item.find(\"span\", class_=\"price\").text.strip()\n",
    "            laptop[\"Product URL\"] = item.find(\"a\")[\"href\"]\n",
    "            laptop_details.append(laptop)\n",
    "\n",
    "        # Create a DataFrame\n",
    "        df = pd.DataFrame(laptop_details)\n",
    "\n",
    "        # Save to CSV\n",
    "        df.to_csv(\"gaming_laptops.csv\", index=False)\n",
    "        print(\"Data saved to gaming_laptops.csv\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {str(e)}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    url = \"[Best Gaming Laptops in India - Digit](https://www.digit.in/top-products/best-gaming-laptops-40.html)\"\n",
    "    scrape_gaming_laptops(url)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dac47b82",
   "metadata": {},
   "source": [
    "7. Write a python program to scrape the details for all billionaires from www.forbes.com. Details to be scrapped: “Rank”, “Name”, “Net worth”, “Age”, “Citizenship”, “Source”, “Industry”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6376897",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "def scrape_billionaires():\n",
    "    try:\n",
    "        url = \"[Forbes Billionaires 2024](https://www.forbes.com/billionaires/)\"\n",
    "        response = requests.get(url)\n",
    "        soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "        # Find all billionaire details\n",
    "        billionaire_details = []\n",
    "        for item in soup.find_all(\"div\", class_=\"personName\"):\n",
    "            billionaire = {}\n",
    "            billionaire[\"Rank\"] = item.find(\"div\", class_=\"rank\").text.strip()\n",
    "            billionaire[\"Name\"] = item.find(\"div\", class_=\"name\").text.strip()\n",
    "            billionaire[\"Net worth\"] = item.find(\"div\", class_=\"netWorth\").text.strip()\n",
    "            billionaire[\"Age\"] = item.find(\"div\", class_=\"age\").text.strip()\n",
    "            billionaire[\"Citizenship\"] = item.find(\"div\", class_=\"countryOfCitizenship\").text.strip()\n",
    "            billionaire[\"Source\"] = item.find(\"div\", class_=\"source\").text.strip()\n",
    "            billionaire[\"Industry\"] = item.find(\"div\", class_=\"category\").text.strip()\n",
    "            billionaire_details.append(billionaire)\n",
    "\n",
    "        # Create a DataFrame\n",
    "        df = pd.DataFrame(billionaire_details)\n",
    "\n",
    "        # Save to CSV\n",
    "        df.to_csv(\"billionaires.csv\", index=False)\n",
    "        print(\"Data saved to billionaires.csv\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {str(e)}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    scrape_billionaires()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "516d83f5",
   "metadata": {},
   "source": [
    "8. Write a program to extract at least 500 Comments, Comment upvote and time when comment was posted from any YouTube Video."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80da1a49",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import json\n",
    "\n",
    "def scrape_youtube_comments(video_id, max_comments=500):\n",
    "    try:\n",
    "        # Set up API request\n",
    "        api_key = \"YOUR_YOUTUBE_API_KEY\"\n",
    "        base_url = \"https://www.googleapis.com/youtube/v3/commentThreads\"\n",
    "        params = {\n",
    "            \"key\": api_key,\n",
    "            \"part\": \"snippet\",\n",
    "            \"videoId\": video_id,\n",
    "            \"maxResults\": 100,  # Maximum results per page\n",
    "        }\n",
    "\n",
    "        # Initialize variables\n",
    "        all_comments = []\n",
    "        next_page_token = None\n",
    "\n",
    "        # Fetch comments until reaching the desired count or no more comments\n",
    "        while len(all_comments) < max_comments:\n",
    "            if next_page_token:\n",
    "                params[\"pageToken\"] = next_page_token\n",
    "\n",
    "            response = requests.get(base_url, params=params)\n",
    "            data = json.loads(response.content)\n",
    "\n",
    "            for item in data.get(\"items\", []):\n",
    "                comment = item[\"snippet\"][\"topLevelComment\"][\"snippet\"]\n",
    "                comment_data = {\n",
    "                    \"Comment\": comment[\"textDisplay\"],\n",
    "                    \"Upvotes\": comment[\"likeCount\"],\n",
    "                    \"Time\": comment[\"publishedAt\"],\n",
    "                }\n",
    "                all_comments.append(comment_data)\n",
    "\n",
    "            next_page_token = data.get(\"nextPageToken\")\n",
    "            if not next_page_token:\n",
    "                break\n",
    "\n",
    "        # Create a DataFrame\n",
    "        df = pd.DataFrame(all_comments)\n",
    "\n",
    "        # Save to CSV\n",
    "        df.to_csv(\"youtube_comments.csv\", index=False)\n",
    "        print(f\"Data saved to youtube_comments.csv\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {str(e)}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    video_id = \"YOUR_YOUTUBE_VIDEO_ID\"\n",
    "    scrape_youtube_comments(video_id, max_comments=500)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e8adca1",
   "metadata": {},
   "source": [
    "9. Write a python program to scrape a data for all available Hostels from https://www.hostelworld.com/ in “London” location. You have to scrape hostel name, distance from city centre, ratings, total reviews, overall reviews, privates from price, dorms from price, facilities and property description."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2415e68f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import json\n",
    "\n",
    "def scrape_youtube_comments(video_id, max_comments=500):\n",
    "    try:\n",
    "        # Set up API request\n",
    "        api_key = \"YOUTUBE_API_KEY\"\n",
    "        base_url = \"https://www.googleapis.com/youtube/v3/commentThreads\"\n",
    "        params = {\n",
    "            \"key\": api_key,\n",
    "            \"part\": \"snippet\",\n",
    "            \"videoId\": video_id,\n",
    "            \"maxResults\": 100,  # Maximum results per page\n",
    "        }\n",
    "\n",
    "        # Initialize variables\n",
    "        all_comments = []\n",
    "        next_page_token = None\n",
    "\n",
    "        # Fetch comments until reaching the desired count or no more comments\n",
    "        while len(all_comments) < max_comments:\n",
    "            if next_page_token:\n",
    "                params[\"pageToken\"] = next_page_token\n",
    "\n",
    "            response = requests.get(base_url, params=params)\n",
    "            data = json.loads(response.content)\n",
    "\n",
    "            for item in data.get(\"items\", []):\n",
    "                comment = item[\"snippet\"][\"topLevelComment\"][\"snippet\"]\n",
    "                comment_data = {\n",
    "                    \"Comment\": comment[\"textDisplay\"],\n",
    "                    \"Upvotes\": comment[\"likeCount\"],\n",
    "                    \"Time\": comment[\"publishedAt\"],\n",
    "                }\n",
    "                all_comments.append(comment_data)\n",
    "\n",
    "            next_page_token = data.get(\"nextPageToken\")\n",
    "            if not next_page_token:\n",
    "                break\n",
    "\n",
    "        # Create a DataFrame\n",
    "        df = pd.DataFrame(all_comments)\n",
    "\n",
    "        # Save to CSV\n",
    "        df.to_csv(\"youtube_comments.csv\", index=False)\n",
    "        print(f\"Data saved to youtube_comments.csv\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {str(e)}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    video_id = \"YOUR_YOUTUBE_VIDEO_ID\"\n",
    "    scrape_youtube_comments(video_id, max_comments=500)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
