{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "98dd7979",
   "metadata": {},
   "source": [
    "# ASSIGNMENT 8 = WEB SCRAPING"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22fec660",
   "metadata": {},
   "source": [
    "# 1. Scrape the details of most viewed videos on YouTube from Wikipedia. Url = https://en.wikipedia.org/wiki/List_of_most-viewed_YouTube_videos You need to find following details:                                               \n",
    "A) Rank\n",
    "B) Name\n",
    "C) Artist\n",
    "D) Upload date\n",
    "E) Views"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2a3dfd5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rank: \"Baby Shark Dance\"[7]\n",
      "Name: Pinkfong Baby Shark - Kids' Songs & Stories\n",
      "Artist: 7,046,700,000\n",
      "Upload Date: November 2, 2020\n",
      "Views: June 17, 2016\n",
      "\n",
      "\n",
      "Rank: \"Despacito\"[10]\n",
      "Name: Luis Fonsi\n",
      "Artist: 2,993,700,000\n",
      "Upload Date: August 4, 2017\n",
      "Views: January 12, 2017\n",
      "\n",
      "\n",
      "Rank: \"See You Again\"[23]\n",
      "Name: Wiz Khalifa\n",
      "Artist: 2,894,000,000\n",
      "Upload Date: July 10, 2017\n",
      "Views: April 6, 2015\n",
      "\n",
      "\n",
      "Rank: \"Gangnam Style\"⁂[31]\n",
      "Name: Psy\n",
      "Artist: 803,700,000\n",
      "Upload Date: November 24, 2012\n",
      "Views: July 15, 2012\n",
      "\n",
      "\n",
      "Rank: \"Baby\"*[69]\n",
      "Name: Justin Bieber\n",
      "Artist: 245,400,000\n",
      "Upload Date: July 16, 2010\n",
      "Views: February 19, 2010\n",
      "\n",
      "\n",
      "Rank: \"Bad Romance\"[73]\n",
      "Name: Lady Gaga\n",
      "Artist: 178,400,000\n",
      "Upload Date: April 14, 2010\n",
      "Views: November 24, 2009\n",
      "\n",
      "\n",
      "Rank: \"Charlie Bit My Finger\"[77]\n",
      "Name: HDCYT\n",
      "Artist: 128,900,000\n",
      "Upload Date: October 25, 2009\n",
      "Views: May 22, 2007\n",
      "\n",
      "\n",
      "Rank: \"Evolution of Dance\"[79]\n",
      "Name: Judson Laipply\n",
      "Artist: 118,900,000\n",
      "Upload Date: May 2, 2009\n",
      "Views: April 6, 2006\n",
      "\n",
      "\n",
      "Rank: \"Girlfriend\"‡[81][82]\n",
      "Name: RCA Records\n",
      "Artist: 92,600,000\n",
      "Upload Date: July 17, 2008\n",
      "Views: February 27, 2007\n",
      "\n",
      "\n",
      "Rank: \"Evolution of Dance\"[79]\n",
      "Name: Judson Laipply\n",
      "Artist: 78,400,000\n",
      "Upload Date: March 15, 2008\n",
      "Views: April 6, 2006\n",
      "\n",
      "\n",
      "Rank: \"Music Is My Hot Hot Sex\"‡[87]\n",
      "Name: CLARUSBARTEL72\n",
      "Artist: 76,600,000\n",
      "Upload Date: March 1, 2008\n",
      "Views: April 9, 2007\n",
      "\n",
      "\n",
      "Rank: \"Evolution of Dance\"*[79]\n",
      "Name: Judson Laipply\n",
      "Artist: 10,600,000\n",
      "Upload Date: May 19, 2006\n",
      "Views: April 6, 2006\n",
      "\n",
      "\n",
      "Rank: \"Pokémon Theme Music Video\"‡[92]\n",
      "Name: Smosh\n",
      "Artist: 4,300,000\n",
      "Upload Date: March 12, 2006\n",
      "Views: November 28, 2005\n",
      "\n",
      "\n",
      "Rank: \"Myspace – The Movie\"‡[97][98]\n",
      "Name: eggtea\n",
      "Artist: 2,700,000\n",
      "Upload Date: February 18, 2006\n",
      "Views: January 31, 2006\n",
      "\n",
      "\n",
      "Rank: \"Phony Photo Booth\"‡[101]\n",
      "Name: mugenized\n",
      "Artist: 3,400,000\n",
      "Upload Date: January 21, 2006\n",
      "Views: December 1, 2005\n",
      "\n",
      "\n",
      "Rank: \"The Chronic of Narnia Rap\"‡[107]\n",
      "Name: youtubedude\n",
      "Artist: 2,300,000\n",
      "Upload Date: January 9, 2006\n",
      "Views: December 18, 2005\n",
      "\n",
      "\n",
      "Rank: \"Ronaldinho: Touch of Gold\"‡*[110]\n",
      "Name: Nikesoccer\n",
      "Artist: 255,000\n",
      "Upload Date: October 31, 2005\n",
      "Views: October 21, 2005\n",
      "\n",
      "\n",
      "Rank: \"I/O Brush\"‡*[116]\n",
      "Name: larfus\n",
      "Artist: 247,000\n",
      "Upload Date: October 29, 2005\n",
      "Views: October 5, 2005\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def scrape_youtube_details():\n",
    "    url = \"https://en.wikipedia.org/wiki/List_of_most-viewed_YouTube_videos\"\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    table = soup.find('table', {'class': 'wikitable sortable'})\n",
    "    rows = table.find_all('tr')[1:]  # Skip the header row\n",
    "\n",
    "    for row in rows:\n",
    "        data = row.find_all('td')\n",
    "        if len(data) < 5:\n",
    "            # Skip rows that don't have enough data\n",
    "            continue\n",
    "        rank = data[0].text.strip()\n",
    "        name = data[1].text.strip()\n",
    "        artist = data[2].text.strip()\n",
    "        upload_date = data[4].text.strip()\n",
    "        views = data[3].text.strip()\n",
    "\n",
    "        print(f\"Rank: {rank}\")\n",
    "        print(f\"Name: {name}\")\n",
    "        print(f\"Artist: {artist}\")\n",
    "        print(f\"Upload Date: {upload_date}\")\n",
    "        print(f\"Views: {views}\")\n",
    "        print(\"\\n\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    scrape_youtube_details()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4074341b",
   "metadata": {},
   "source": [
    "# 2. Scrape the details team India’s international fixtures from bcci.tv.\n",
    "Url = https://www.bcci.tv/.\n",
    "You need to find following details:\n",
    "A) Series\n",
    "B) Place\n",
    "C) Date\n",
    "D) Time\n",
    "Note: - From bcci.tv home page you have reach to the international fixture page through code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ab20181",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin\n",
    "\n",
    "def scrape_bcci_fixtures():\n",
    "    base_url = \"https://www.bcci.tv/\"\n",
    "    response = requests.get(base_url)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # Find the link to the international fixtures page and construct the full URL\n",
    "    fixtures_link = soup.find('a', {'class': 'navigation__link navigation__link--in-drop-down', 'href': '/international/fixtures'})\n",
    "    fixtures_url = urljoin(base_url, fixtures_link['href'])\n",
    "\n",
    "    # Send a request to the international fixtures page\n",
    "    response = requests.get(fixtures_url)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # Find all fixture blocks\n",
    "    fixture_blocks = soup.find_all('div', {'class': 'js-list'})\n",
    "\n",
    "    for block in fixture_blocks:\n",
    "        series = block.find('span', {'class': 'u-unskewed-text fixture__tournament-label'}).text.strip()\n",
    "        place = block.find('p', {'class': 'fixture__additional-info'}).text.strip()\n",
    "        date = block.find('div', {'class': 'fixture__datetime desktop-only'}).find_all('span')[0].text.strip()\n",
    "        time = block.find('div', {'class': 'fixture__datetime desktop-only'}).find_all('span')[1].text.strip()\n",
    "\n",
    "        print(f\"Series: {series}\")\n",
    "        print(f\"Place: {place}\")\n",
    "        print(f\"Date: {date}\")\n",
    "        print(f\"Time: {time}\")\n",
    "        print(\"\\n\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    scrape_bcci_fixtures()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c20e689c",
   "metadata": {},
   "source": [
    "# 3. Scrape the details of State-wise GDP of India from statisticstime.com.\n",
    "Url = http://statisticstimes.com/\n",
    "You have to find following details: A) Rank\n",
    "B) State\n",
    "C) GSDP(18-19)- at current prices\n",
    "D) GSDP(19-20)- at current prices\n",
    "E) Share(18-19)\n",
    "F) GDP($ billion)\n",
    "Note: - From statisticstimes home page you have to reach to economy page through code.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12a9fd00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# URL to the economy page on StatisticsTimes\n",
    "url = \"http://statisticstimes.com/economy/india/indian-states-gdp.php\"\n",
    "\n",
    "# Send an HTTP request and get the page content\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "# Find the table containing the GDP details\n",
    "gdp_table = soup.find(\"table\", class_=\"display compact\")\n",
    "\n",
    "# Initialize lists to store the extracted data\n",
    "rank_list = []\n",
    "state_list = []\n",
    "gsdp_18_19_list = []\n",
    "gsdp_19_20_list = []\n",
    "share_18_19_list = []\n",
    "gdp_billion_list = []\n",
    "\n",
    "# Extract data from each row in the table\n",
    "for row in gdp_table.find_all(\"tr\")[1:]:\n",
    "    columns = row.find_all(\"td\")\n",
    "    rank_list.append(columns[0].text.strip())\n",
    "    state_list.append(columns[1].text.strip())\n",
    "    gsdp_18_19_list.append(columns[2].text.strip())\n",
    "    gsdp_19_20_list.append(columns[3].text.strip())\n",
    "    share_18_19_list.append(columns[4].text.strip())\n",
    "    gdp_billion_list.append(columns[5].text.strip())\n",
    "\n",
    "# Print the extracted data\n",
    "for i in range(len(rank_list)):\n",
    "    print(f\"Rank {rank_list[i]}: {state_list[i]}\")\n",
    "    print(f\"GSDP(18-19) at current prices: ₹{gsdp_18_19_list[i]} crore\")\n",
    "    print(f\"GSDP(19-20) at current prices: ₹{gsdp_19_20_list[i]} crore\")\n",
    "    print(f\"Share(18-19): {share_18_19_list[i]}%\")\n",
    "    print(f\"GDP($ billion): {gdp_billion_list[i]}\")\n",
    "    print(\"-\" * 40)\n",
    "\n",
    "# Note: The data is based on the fiscal year 2021-22.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b53a81ce",
   "metadata": {},
   "source": [
    "# 4. Scrape the details of trending repositories on Github.com.\n",
    "Url = https://github.com/\n",
    "You have to find the following details:\n",
    "A) Repository title\n",
    "B) Repository description\n",
    "C) Contributors count\n",
    "D) Language used\n",
    "Note: - From the home page you have to click on the trending option from Explore menu through code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bf507a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# URL for GitHub's trending repositories\n",
    "url = \"https://github.com/trending\"\n",
    "\n",
    "# Send an HTTP request and get the page content\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "# Find the list of trending repositories\n",
    "repo_list = soup.find_all(\"article\", class_=\"Box-row\")\n",
    "\n",
    "# Extract details for each repository\n",
    "for repo in repo_list:\n",
    "    title = repo.find(\"h1\").text.strip()\n",
    "    description = repo.find(\"p\", class_=\"col-9\").text.strip()\n",
    "    contributors_count = repo.find(\"a\", href=True, text=True, class_=\"muted-link\").text.strip()\n",
    "    language = repo.find(\"span\", class_=\"d-inline-block ml-0 mr-3\").text.strip()\n",
    "\n",
    "    print(f\"Repository Title: {title}\")\n",
    "    print(f\"Description: {description}\")\n",
    "    print(f\"Contributors Count: {contributors_count}\")\n",
    "    print(f\"Language Used: {language}\")\n",
    "    print(\"-\" * 40)\n",
    "\n",
    "# Note: The data is based on the current trending repositories.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daaa16e8",
   "metadata": {},
   "source": [
    "# 5. Scrape the details of top 100 songs on billiboard.com. Url = https:/www.billboard.com/ You have to find the\n",
    "following details:\n",
    "A) Song name\n",
    "B) Artist name\n",
    "C) Last week rank\n",
    "D) Peak rank\n",
    "E) Weeks on board\n",
    "Note: - From the home page you have to click on the charts option then hot 100-page link through code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4864f219",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# URL for the Billboard Hot 100 page\n",
    "url = \"https://www.billboard.com/charts/hot-100\"\n",
    "\n",
    "# Send an HTTP request and get the page content\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "# Find the list of songs\n",
    "song_list = soup.find_all(\"li\", class_=\"chart-list__element\")\n",
    "\n",
    "# Extract details for each song\n",
    "for song in song_list:\n",
    "    song_name = song.find(\"span\", class_=\"chart-element__information__song\").text.strip()\n",
    "    artist_name = song.find(\"span\", class_=\"chart-element__information__artist\").text.strip()\n",
    "    last_week_rank = song.find(\"span\", class_=\"chart-element__meta text--last\").text.strip()\n",
    "    peak_rank = song.find(\"span\", class_=\"chart-element__meta text--peak\").text.strip()\n",
    "    weeks_on_board = song.find(\"span\", class_=\"chart-element__meta text--week\").text.strip()\n",
    "\n",
    "    print(f\"Song Name: {song_name}\")\n",
    "    print(f\"Artist Name: {artist_name}\")\n",
    "    print(f\"Last Week Rank: {last_week_rank}\")\n",
    "    print(f\"Peak Rank: {peak_rank}\")\n",
    "    print(f\"Weeks on Board: {weeks_on_board}\")\n",
    "    print(\"-\" * 40)\n",
    "\n",
    "# Note: The data reflects the current Billboard Hot 100 chart.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99f65495",
   "metadata": {},
   "source": [
    "# 6. Scrape the details of Highest selling novels.\n",
    "A) Book name\n",
    "B) Author name\n",
    "C) Volumes sold\n",
    "D) Publisher\n",
    "E) Genre\n",
    "Url - https://www.theguardian.com/news/datablog/2012/aug/09/best-selling-books-all-time-fifty-shades-grey-compare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ef06bc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# URL for The Guardian's list of top-selling books\n",
    "url = \"https://www.theguardian.com/news/datablog/2012/aug/09/best-selling-books-all-time-fifty-shades-grey-compare\"\n",
    "\n",
    "# Send an HTTP request and get the page content\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "# Find the table containing the book details\n",
    "book_table = soup.find(\"table\", class_=\"in-article sortable\")\n",
    "\n",
    "# Initialize lists to store the extracted data\n",
    "book_names = []\n",
    "author_names = []\n",
    "volumes_sold = []\n",
    "publishers = []\n",
    "genres = []\n",
    "\n",
    "# Extract data from each row in the table\n",
    "for row in book_table.find_all(\"tr\")[1:]:\n",
    "    columns = row.find_all(\"td\")\n",
    "    book_names.append(columns[0].text.strip())\n",
    "    author_names.append(columns[1].text.strip())\n",
    "    volumes_sold.append(columns[2].text.strip())\n",
    "    publishers.append(columns[3].text.strip())\n",
    "    genres.append(columns[4].text.strip())\n",
    "\n",
    "# Print the extracted data\n",
    "for i in range(len(book_names)):\n",
    "    print(f\"Book Name: {book_names[i]}\")\n",
    "    print(f\"Author Name: {author_names[i]}\")\n",
    "    print(f\"Volumes Sold: {volumes_sold[i]}\")\n",
    "    print(f\"Publisher: {publishers[i]}\")\n",
    "    print(f\"Genre: {genres[i]}\")\n",
    "    print(\"-\" * 40)\n",
    "\n",
    "# Note: The data is based on records up to 2012.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c63a7df2",
   "metadata": {},
   "source": [
    "# 7. Scrape the details most watched tv series of all time from imdb.com.\n",
    "Url = \"https://www.imdb.com/list/ls512407256/\". You have\n",
    "to find the following details:\n",
    "A) Name\n",
    "B) Year span\n",
    "C) Genre\n",
    "D) Run time\n",
    "E) Ratings\n",
    "F) Votes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c2b61bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# URL for IMDb's top 250 TV shows\n",
    "url = \"https://www.imdb.com/list/ls512407256/\"\n",
    "\n",
    "# Send an HTTP request and get the page content\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "# Initialize lists to store data\n",
    "names = []\n",
    "year_spans = []\n",
    "genres = []\n",
    "run_times = []\n",
    "ratings = []\n",
    "votes = []\n",
    "\n",
    "# Find the list of TV series\n",
    "series_list = soup.find_all(\"div\", class_=\"lister-item-content\")\n",
    "\n",
    "# Extract details for each series\n",
    "for series in series_list:\n",
    "    name = series.h3.a.text.strip()\n",
    "    year_span = series.find(\"span\", class_=\"lister-item-year\").text.strip()\n",
    "    genre = series.find(\"span\", class_=\"genre\").text.strip()\n",
    "    run_time = series.find(\"span\", class_=\"runtime\").text.strip()\n",
    "    rating = float(series.strong.text.strip())\n",
    "    vote_count = int(series.find(\"span\", attrs={\"name\": \"nv\"})[\"data-value\"])\n",
    "\n",
    "    names.append(name)\n",
    "    year_spans.append(year_span)\n",
    "    genres.append(genre)\n",
    "    run_times.append(run_time)\n",
    "    ratings.append(rating)\n",
    "    votes.append(vote_count)\n",
    "\n",
    "# Create a DataFrame to organize the data\n",
    "df = pd.DataFrame({\n",
    "    \"Name\": names,\n",
    "    \"Year Span\": year_spans,\n",
    "    \"Genre\": genres,\n",
    "    \"Run Time\": run_times,\n",
    "    \"Ratings\": ratings,\n",
    "    \"Votes\": votes\n",
    "})\n",
    "\n",
    "# Save the data to a CSV file\n",
    "df.to_csv(\"imdb_top_tv_series.csv\", index=False)\n",
    "\n",
    "print(\"Data scraped successfully! Check the 'imdb_top_tv_series.csv' file.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1231622",
   "metadata": {},
   "source": [
    "# 8. Find details of datasets from UCI machine learning repositories.\n",
    "Url = https://archive.ics.uci.edu/ You\n",
    "have to find the following details:\n",
    "A) Dataset name\n",
    "B) Data type\n",
    "C) Task\n",
    "D) Attribute type\n",
    "E) No of instances\n",
    "F) No of attribute G) Year\n",
    "Note: - from the home page you have to go to the Show All Dataset page through code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a41383d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# URL of the UCI ML Repository's 'All Datasets' page\n",
    "url = \"https://archive.ics.uci.edu/ml/datasets.php\"\n",
    "\n",
    "# Send a GET request\n",
    "response = requests.get(url)\n",
    "\n",
    "# Parse the HTML content of the page with BeautifulSoup\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "# Find the table containing the dataset details\n",
    "table = soup.find('table', border=\"1\")\n",
    "\n",
    "# Extract the table headers\n",
    "headers = [header.text for header in table.find_all('th')]\n",
    "\n",
    "# Extract the details for each dataset\n",
    "datasets = []\n",
    "for row in table.find_all('tr'):\n",
    "    dataset = {}\n",
    "    for header, cell in zip(headers, row.find_all('td')):\n",
    "        dataset[header] = cell.text\n",
    "    datasets.append(dataset)\n",
    "\n",
    "# Print the details of each dataset\n",
    "for dataset in datasets:\n",
    "    print(f\"Dataset Name: {dataset['Name']}\")\n",
    "    print(f\"Data Type: {dataset['Data Types']}\")\n",
    "    print(f\"Default Task: {dataset['Default Task']}\")\n",
    "    print(f\"Attribute Types: {dataset['Attribute Types']}\")\n",
    "    print(f\"No of Instances: {dataset['# Instances']}\")\n",
    "    print(f\"No of Attributes: {dataset['# Attributes']}\")\n",
    "    print(f\"Year: {dataset['Year']}\")\n",
    "    print(\"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
